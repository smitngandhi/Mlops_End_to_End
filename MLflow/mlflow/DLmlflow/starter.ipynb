{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4325b22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hyperopt import STATUS_OK , Trials , hp ,fmin , Trials , tpe\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc50d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.00100</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.99400</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.99510</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4898 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.0              0.27         0.36            20.7      0.045   \n",
       "1               6.3              0.30         0.34             1.6      0.049   \n",
       "2               8.1              0.28         0.40             6.9      0.050   \n",
       "3               7.2              0.23         0.32             8.5      0.058   \n",
       "4               7.2              0.23         0.32             8.5      0.058   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "4893            6.2              0.21         0.29             1.6      0.039   \n",
       "4894            6.6              0.32         0.36             8.0      0.047   \n",
       "4895            6.5              0.24         0.19             1.2      0.041   \n",
       "4896            5.5              0.29         0.30             1.1      0.022   \n",
       "4897            6.0              0.21         0.38             0.8      0.020   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    45.0                 170.0  1.00100  3.00       0.45   \n",
       "1                    14.0                 132.0  0.99400  3.30       0.49   \n",
       "2                    30.0                  97.0  0.99510  3.26       0.44   \n",
       "3                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "4                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "4893                 24.0                  92.0  0.99114  3.27       0.50   \n",
       "4894                 57.0                 168.0  0.99490  3.15       0.46   \n",
       "4895                 30.0                 111.0  0.99254  2.99       0.46   \n",
       "4896                 20.0                 110.0  0.98869  3.34       0.38   \n",
       "4897                 22.0                  98.0  0.98941  3.26       0.32   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         8.8        6  \n",
       "1         9.5        6  \n",
       "2        10.1        6  \n",
       "3         9.9        6  \n",
       "4         9.9        6  \n",
       "...       ...      ...  \n",
       "4893     11.2        6  \n",
       "4894      9.6        5  \n",
       "4895      9.4        6  \n",
       "4896     12.8        7  \n",
       "4897     11.8        6  \n",
       "\n",
       "[4898 rows x 12 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/mlflow/mlflow/master/tests/datasets/winequality-white.csv\",\n",
    "    sep=\";\",\n",
    ")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "63a94c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['quality'] , axis = 1)\n",
    "y = data['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c65aa7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_test , y_train , y_test = train_test_split(X , y , test_size = 0.2 , random_state=101)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f7d0ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , X_valid , y_train , y_valid = train_test_split(X_train , y_train , test_size = 0.2 , random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b93f8b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1866    6\n",
       "1366    6\n",
       "253     3\n",
       "4217    4\n",
       "1066    6\n",
       "       ..\n",
       "3568    7\n",
       "3829    7\n",
       "4831    6\n",
       "3191    6\n",
       "4849    5\n",
       "Name: quality, Length: 3134, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b28f3c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3134,)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f7d5c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_valid = np.array(X_valid)\n",
    "y_valid = np.array(y_valid)\n",
    "X_test = np.array(X_test)\n",
    "y_test =np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2402d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1 , 1).flatten()\n",
    "y_valid = y_valid.reshape(-1 , 1).flatten()\n",
    "y_test = y_test.reshape(-1 , 1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6beb1843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train and y_train shape are (3134, 11) and (3134,)\n",
      "X_valid and y_valid shape are (784, 11) and (784,)\n",
      "X_test and y_test shape are (980, 11) and (980,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train and y_train shape are {X_train.shape} and {y_train.shape}\")\n",
    "print(f\"X_valid and y_valid shape are {X_valid.shape} and {y_valid.shape}\")\n",
    "print(f\"X_test and y_test shape are {X_test.shape} and {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "90ee7a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 6, 3, ..., 6, 6, 5])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4519056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature = infer_signature(X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "728495ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input , Normalization , Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import RootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "dff64a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train , y_train , X_valid , y_valid , X_test , y_test , params , epochs):\n",
    "\n",
    "    mean = np.mean(X_train , axis = 0)\n",
    "    var = np.var(X_train , axis = 0)\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(Input([X_train.shape[1]]))\n",
    "    model.add(Normalization(mean=mean , variance=var))\n",
    "    model.add(Dense(64 , activation ='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    metric = RootMeanSquaredError()\n",
    "    model.compile(optimizer=Adam(learning_rate=params[\"lr\"]) , loss=\"mean_squared_error\" , metrics=[metric])\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        model.fit(X_train , y_train , validation_data=(X_valid , y_valid) , epochs = epochs , batch_size=32)\n",
    "\n",
    "        eval_model = model.evaluate(X_test , y_test)\n",
    "        eval_rmse = eval_model[1]\n",
    "\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"eval_rmse\" , eval_rmse)\n",
    "\n",
    "        mlflow.tensorflow.log_model(model ,\"model\" , signature=signature)\n",
    "\n",
    "        return {\"loss\" : eval_rmse , \"status\" : STATUS_OK , \"model\" : model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8c64ee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    # MLflow will track the parameters and results for each run\n",
    "    result = train_model(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_valid,\n",
    "        y_valid,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        params,\n",
    "        3\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "346c1920",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    \"lr\" : hp.loguniform(\"lr\", np.log(1e-5) , np.log(1e-2))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2badc6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/20 22:38:07 INFO mlflow.tracking.fluent: Experiment with name 'Wine Quality' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3                                            \n",
      "\n",
      "\u001b[1m 1/98\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 658ms/step - loss: 37.8328 - root_mean_squared_error: 6.1508\n",
      "\u001b[1m22/98\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 38.6682 - root_mean_squared_error: 6.2183    \n",
      "\u001b[1m45/98\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 38.1210 - root_mean_squared_error: 6.1740\n",
      "\u001b[1m71/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 37.4164 - root_mean_squared_error: 6.1162\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 36.7086 - root_mean_squared_error: 6.0575\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 36.6837 - root_mean_squared_error: 6.0554 - val_loss: 29.3793 - val_root_mean_squared_error: 5.4203\n",
      "\n",
      "Epoch 2/3                                            \n",
      "\n",
      "\u001b[1m 1/98\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 51ms/step - loss: 28.5233 - root_mean_squared_error: 5.3407\n",
      "\u001b[1m21/98\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 28.7874 - root_mean_squared_error: 5.3653 \n",
      "\u001b[1m37/98\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 28.6769 - root_mean_squared_error: 5.3550\n",
      "\u001b[1m55/98\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 28.4121 - root_mean_squared_error: 5.3301\n",
      "\u001b[1m70/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 28.1018 - root_mean_squared_error: 5.3006\n",
      "\u001b[1m93/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 27.5796 - root_mean_squared_error: 5.2505\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 27.4424 - root_mean_squared_error: 5.2373 - val_loss: 21.3264 - val_root_mean_squared_error: 4.6181\n",
      "\n",
      "Epoch 3/3                                            \n",
      "\n",
      "\u001b[1m 1/98\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - loss: 21.0709 - root_mean_squared_error: 4.5903\n",
      "\u001b[1m24/98\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 20.5026 - root_mean_squared_error: 4.5278 \n",
      "\u001b[1m47/98\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 20.1353 - root_mean_squared_error: 4.4869\n",
      "\u001b[1m70/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 19.7902 - root_mean_squared_error: 4.4480\n",
      "\u001b[1m90/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 19.4922 - root_mean_squared_error: 4.4141\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 19.3541 - root_mean_squared_error: 4.3982 - val_loss: 14.7413 - val_root_mean_squared_error: 3.8394\n",
      "\n",
      "\u001b[1m 1/31\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 14.6419 - root_mean_squared_error: 3.8265\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13.9878 - root_mean_squared_error: 3.7397 \n",
      "\n",
      "🏃 View run adorable-bug-868 at: http://127.0.0.1:5000/#/experiments/342870525102222293/runs/a83ea99dbf4d47d39f3971ca38469a64\n",
      "\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/342870525102222293\n",
      "\n",
      "Epoch 1/3                                                                     \n",
      "\n",
      "\u001b[1m 1/98\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 640ms/step - loss: 40.6949 - root_mean_squared_error: 6.3793\n",
      "\u001b[1m39/98\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 41.5221 - root_mean_squared_error: 6.4437    \n",
      "\u001b[1m74/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 41.6845 - root_mean_squared_error: 6.4563\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 41.6698 - root_mean_squared_error: 6.4552 - val_loss: 41.4525 - val_root_mean_squared_error: 6.4384\n",
      "\n",
      "Epoch 2/3                                                                     \n",
      "\n",
      "\u001b[1m 1/98\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 46ms/step - loss: 41.0778 - root_mean_squared_error: 6.4092\n",
      "\u001b[1m20/98\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 40.9714 - root_mean_squared_error: 6.4008 \n",
      "\u001b[1m39/98\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 41.0830 - root_mean_squared_error: 6.4095\n",
      "\u001b[1m57/98\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 41.0501 - root_mean_squared_error: 6.4070\n",
      "\u001b[1m72/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 41.0370 - root_mean_squared_error: 6.4060\n",
      "\u001b[1m91/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 41.0393 - root_mean_squared_error: 6.4062\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 41.0395 - root_mean_squared_error: 6.4062 - val_loss: 40.8784 - val_root_mean_squared_error: 6.3936\n",
      "\n",
      "Epoch 3/3                                                                     \n",
      "\n",
      "\u001b[1m 1/98\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 65ms/step - loss: 39.6242 - root_mean_squared_error: 6.2948\n",
      "\u001b[1m20/98\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 39.9443 - root_mean_squared_error: 6.3199 \n",
      "\u001b[1m42/98\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 40.4683 - root_mean_squared_error: 6.3612\n",
      "\u001b[1m62/98\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 40.5816 - root_mean_squared_error: 6.3702\n",
      "\u001b[1m83/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 40.6016 - root_mean_squared_error: 6.3718\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 40.5893 - root_mean_squared_error: 6.3709 - val_loss: 40.3098 - val_root_mean_squared_error: 6.3490\n",
      "\n",
      "\u001b[1m 1/31\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 40.5047 - root_mean_squared_error: 6.3643\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39.3467 - root_mean_squared_error: 6.2726 \n",
      "\n",
      "🏃 View run ambitious-horse-990 at: http://127.0.0.1:5000/#/experiments/342870525102222293/runs/a28eae84bad84bb6bf11250894624b17\n",
      "\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/342870525102222293 \n",
      "\n",
      "Epoch 1/3                                                                     \n",
      "\n",
      "\u001b[1m 1/98\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 526ms/step - loss: 38.0100 - root_mean_squared_error: 6.1652\n",
      "\u001b[1m34/98\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 29.0689 - root_mean_squared_error: 5.3691   \n",
      "\u001b[1m70/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 22.0380 - root_mean_squared_error: 4.6201\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 18.6541 - root_mean_squared_error: 4.2146 - val_loss: 2.2341 - val_root_mean_squared_error: 1.4947\n",
      "\n",
      "Epoch 2/3                                                                     \n",
      "\n",
      "\u001b[1m 1/98\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 38ms/step - loss: 2.8291 - root_mean_squared_error: 1.6820\n",
      "\u001b[1m34/98\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1783 - root_mean_squared_error: 1.4736 \n",
      "\u001b[1m68/98\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.0411 - root_mean_squared_error: 1.4266\n",
      "\u001b[1m91/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9732 - root_mean_squared_error: 1.4025\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9553 - root_mean_squared_error: 1.3961 - val_loss: 1.2948 - val_root_mean_squared_error: 1.1379\n",
      "\n",
      "Epoch 3/3                                                                     \n",
      "\n",
      "\u001b[1m 1/98\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 65ms/step - loss: 1.2556 - root_mean_squared_error: 1.1205\n",
      "\u001b[1m13/98\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.2543 - root_mean_squared_error: 1.1198 \n",
      "\u001b[1m26/98\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.2574 - root_mean_squared_error: 1.1212\n",
      "\u001b[1m41/98\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.2556 - root_mean_squared_error: 1.1204\n",
      "\u001b[1m59/98\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.2331 - root_mean_squared_error: 1.1103\n",
      "\u001b[1m79/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2024 - root_mean_squared_error: 1.0961\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1737 - root_mean_squared_error: 1.0827 - val_loss: 0.8361 - val_root_mean_squared_error: 0.9144\n",
      "\n",
      "\u001b[1m 1/31\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - loss: 1.4582 - root_mean_squared_error: 1.2076\n",
      "\u001b[1m26/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9596 - root_mean_squared_error: 0.9781 \n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9435 - root_mean_squared_error: 0.9700\n",
      "\n",
      "🏃 View run ambitious-turtle-221 at: http://127.0.0.1:5000/#/experiments/342870525102222293/runs/3042c83a09614a2297aaa7196c157a0b\n",
      "\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/342870525102222293 \n",
      "\n",
      "100%|██████████| 3/3 [00:40<00:00, 13.53s/trial, best loss: 0.9318456053733826]\n",
      "Best parameters: {'lr': np.float64(0.00462938172175095)}\n",
      "Best eval rmse: 0.9318456053733826\n",
      "🏃 View run bustling-mink-45 at: http://127.0.0.1:5000/#/experiments/342870525102222293/runs/53bd5d1d15d74d49b87cfebc084baa30\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/342870525102222293\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"Wine Quality\")\n",
    "with mlflow.start_run():\n",
    "\n",
    "    trial = Trials()\n",
    "    best = fmin(\n",
    "        fn = objective,\n",
    "        space = space ,\n",
    "        trials = trial,\n",
    "        algo = tpe.suggest,\n",
    "        max_evals = 3\n",
    "    )\n",
    "\n",
    "    best_run = sorted(trial.results , key=lambda x: x[\"loss\"])[0]\n",
    "\n",
    "    mlflow.log_params(best)\n",
    "    mlflow.log_metric(\"eval_rmse\" , best_run[\"loss\"])\n",
    "    mlflow.tensorflow.log_model(best_run[\"model\"] , \"model\" , signature = signature)\n",
    "\n",
    "    print(f\"Best parameters: {best}\")\n",
    "    print(f\"Best eval rmse: {best_run['loss']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7c3a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/20 23:05:15 INFO mlflow.models.python_api: It is highly recommended to use `uv` as the environment manager for predicting with MLflow models as its performance is significantly better than other environment managers. Run `pip install uv` to install uv. See https://docs.astral.sh/uv/getting-started/installation for other installation methods.\n",
      "Downloading artifacts: 100%|██████████| 7/7 [00:00<00:00, 60.79it/s]  \n",
      "2025/05/20 23:05:15 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor 'python_function'\n",
      "Downloading artifacts: 100%|██████████| 7/7 [00:00<00:00, 54.94it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "{\"predictions\": [[5.469059467315674], [6.422222137451172], [6.413414001464844], [5.503286361694336], [5.267496109008789], [6.1405229568481445], [5.412426948547363], [5.712819576263428], [5.832468032836914], [5.370978355407715], [5.638043403625488], [5.744564056396484], [6.315342426300049], [6.073831558227539], [5.363818168640137], [8.270758628845215], [5.764993667602539], [6.188596248626709], [5.603005409240723], [6.166471481323242], [5.441128730773926], [5.6060709953308105], [4.633454322814941], [5.079880714416504], [7.158854007720947], [6.52211332321167], [6.371703147888184], [5.99822998046875], [5.943861961364746], [5.508855819702148], [5.80950927734375], [5.958525657653809], [5.673552513122559], [5.527407646179199], [5.448401927947998], [5.347448348999023], [5.5475945472717285], [6.595968246459961], [6.070286750793457], [5.946592807769775], [4.935441493988037], [6.7395100593566895], [6.7451019287109375], [5.478515625], [5.822560787200928], [5.348295211791992], [5.595751762390137], [6.483135223388672], [5.598748683929443], [6.7829179763793945], [5.734711170196533], [5.569664478302002], [5.793941020965576], [5.441351890563965], [5.194363594055176], [5.63411283493042], [5.5327301025390625], [7.0627217292785645], [5.870835304260254], [7.0970540046691895], [4.765654563903809], [4.975306510925293], [5.162332534790039], [4.813875198364258], [4.976879596710205], [6.097722053527832], [5.864887237548828], [6.888341903686523], [5.915576934814453], [6.228726387023926], [5.255699634552002], [6.102153778076172], [5.909456729888916], [6.067112922668457], [5.7296037673950195], [6.311168670654297], [6.572608947753906], [5.717211723327637], [6.311142444610596], [5.436144828796387], [5.982693672180176], [7.088689804077148], [6.851345062255859], [5.905466556549072], [5.014364242553711], [6.4227213859558105], [5.957703590393066], [6.413406848907471], [6.287374973297119], [6.623712062835693], [6.407476902008057], [5.730766296386719], [6.251996040344238], [4.707667350769043], [7.200663089752197], [5.471529006958008], [6.481352806091309], [5.905466556549072], [5.650845050811768], [6.576904296875], [6.011521339416504], [5.105257511138916], [6.448397636413574], [6.456321716308594], [5.971449851989746], [5.909077167510986], [5.005964279174805], [5.842341423034668], [5.86252498626709], [6.115395545959473], [6.058657169342041], [5.223923683166504], [6.837680339813232], [5.206517219543457], [5.6269917488098145], [6.992067813873291], [5.411063194274902], [6.87082576751709], [6.356661319732666], [5.347638130187988], [5.407214164733887], [4.766169548034668], [6.234892845153809], [5.93575382232666], [4.316023826599121], [6.11068058013916], [5.195281982421875], [5.146512508392334], [6.106974124908447], [6.693650245666504], [6.572312355041504], [6.27944278717041], [4.35264778137207], [5.047825813293457], [5.17972993850708], [5.320621490478516], [5.057679176330566], [6.3561015129089355], [4.8910980224609375], [6.450429916381836], [5.469700813293457], [4.719489097595215], [6.73709774017334], [5.588720798492432], [5.521965980529785], [5.348543167114258], [6.368535995483398], [5.479978561401367], [5.9571852684021], [5.321675777435303], [5.256630897521973], [5.619001865386963], [5.892393589019775], [6.220560073852539], [5.629220485687256], [5.517308712005615], [5.004399299621582], [6.686344623565674], [6.574543476104736], [5.833156585693359], [6.037991523742676], [6.547311782836914], [5.3479413986206055], [5.519813537597656], [5.876473426818848], [6.490152835845947], [5.903252601623535], [6.049682140350342], [5.166398048400879], [6.047650337219238], [5.770382881164551], [5.244265556335449], [4.756210803985596], [6.0216827392578125], [5.81120491027832], [5.645014762878418], [4.999486923217773], [5.135657787322998], [5.924150466918945], [6.193397045135498], [5.208101749420166], [5.452892303466797], [6.844992637634277], [6.233462810516357], [5.150766849517822], [6.632511615753174], [5.4500508308410645], [5.077358245849609], [4.905246734619141], [6.955079078674316], [5.423523902893066], [6.62989616394043], [5.835173606872559], [5.519103050231934], [5.57226037979126], [7.2388410568237305], [5.823324680328369], [6.066356658935547], [6.274852275848389], [5.633761405944824], [6.689868927001953], [5.305158615112305], [6.613117218017578], [5.736400604248047], [6.049682140350342], [6.325533390045166], [5.688661575317383], [5.669247627258301], [5.548336029052734], [6.721719264984131], [4.704448699951172], [5.922610282897949], [5.143850326538086], [5.953493595123291], [7.39261531829834], [5.09349250793457], [5.796165466308594], [6.3828582763671875], [6.323564529418945], [4.826971054077148], [5.938172340393066], [6.657720565795898], [6.578279495239258], [4.633454322814941], [5.419122695922852], [6.051762580871582], [5.719353675842285], [4.777009010314941], [5.199803352355957], [4.845470428466797], [6.745772361755371], [4.945487022399902], [5.421816825866699], [5.280242919921875], [6.468647480010986], [5.869049549102783], [5.148538589477539], [5.922610282897949], [6.672054767608643], [6.857922554016113], [6.745772361755371], [5.095906734466553], [6.0728044509887695], [6.053579330444336], [6.191262722015381], [5.545755386352539], [6.871105194091797], [6.108944892883301], [5.744261264801025], [5.328776836395264], [5.367647171020508], [5.545480728149414], [6.29835319519043], [5.626040458679199], [5.24310302734375], [5.698403835296631], [4.92495584487915], [6.695479869842529], [5.758892059326172], [5.322275638580322], [7.432242393493652], [4.844672679901123], [6.416947364807129], [7.490298271179199], [6.139333248138428], [5.826581001281738], [5.1699628829956055], [4.440157413482666], [6.462486267089844], [5.884798049926758], [7.225344657897949], [5.791881084442139], [5.010332107543945], [5.534650802612305], [6.4340949058532715], [4.893368721008301], [5.664484977722168], [5.167090892791748], [7.0177178382873535], [7.589697360992432], [6.194631576538086], [6.531480312347412], [5.500605583190918], [5.770792007446289], [6.110797882080078], [5.884647846221924], [5.651054382324219], [6.622372627258301], [5.394100666046143], [4.977691650390625], [5.013645172119141], [6.181399822235107], [6.1761274337768555], [5.807623863220215], [5.202259063720703], [5.727004051208496], [7.2252655029296875], [5.375681400299072], [6.038600444793701], [6.28955078125], [5.231237411499023], [5.30710506439209], [5.631021499633789], [4.473034858703613], [6.522446155548096], [6.266223907470703], [5.942963600158691], [6.815607070922852], [5.915940284729004], [4.632061958312988], [8.344948768615723], [6.548388481140137], [6.296196937561035], [6.8436737060546875], [6.628501892089844], [6.108518600463867], [6.404515266418457], [5.672083854675293], [5.8710408210754395], [5.208102226257324], [5.991117000579834], [5.005130767822266], [6.090148448944092], [6.0175089836120605], [5.587423324584961], [7.242165565490723], [5.716118812561035], [5.958569526672363], [5.85706901550293], [5.455519199371338], [6.237764358520508], [4.942673683166504], [7.491888046264648], [6.518892288208008], [6.388617038726807], [6.203869819641113], [6.094397068023682], [6.572608947753906], [5.2931342124938965], [5.591627597808838], [6.956289768218994], [5.568609237670898], [7.62336540222168], [5.788704872131348], [5.400739669799805], [5.852597713470459], [6.2436957359313965], [5.399247169494629], [8.445996284484863], [7.113773822784424], [5.367107391357422], [4.794436931610107], [5.073144435882568], [5.643154144287109], [5.3007493019104], [6.154365062713623], [6.190361022949219], [5.210886478424072], [6.651138782501221], [5.373218059539795], [5.619001865386963], [6.040757179260254], [5.251107215881348], [6.188117980957031], [4.834402084350586], [6.05029296875], [5.02119779586792], [5.731631278991699], [4.670831203460693], [6.157851219177246], [5.513815879821777], [6.375030994415283], [6.2885050773620605], [5.479427814483643], [5.987329006195068], [5.384924411773682], [6.248440265655518], [5.429791450500488], [4.484052658081055], [4.473034858703613], [5.445534706115723], [6.427543640136719], [5.833156585693359], [6.137329578399658], [5.271856784820557], [4.858581066131592], [6.572739601135254], [5.207742691040039], [5.053927421569824], [6.200381278991699], [5.535891532897949], [4.912555694580078], [5.09349250793457], [5.409400463104248], [4.70068359375], [6.482649803161621], [5.905466556549072], [6.211173057556152], [6.161194801330566], [4.867966175079346], [6.543193340301514], [4.9898681640625], [5.73614501953125], [6.353850364685059], [5.457236289978027], [6.541726112365723], [6.5074543952941895], [5.754309177398682], [5.047255516052246], [5.0753679275512695], [4.935622215270996], [6.432767868041992], [6.0291056632995605], [5.754731178283691], [5.8633270263671875], [6.1666741371154785], [5.828700065612793], [6.4688215255737305], [7.031302452087402], [6.3217339515686035], [6.311578750610352], [6.062488555908203], [4.654998779296875], [5.887263298034668], [5.320586204528809], [7.566114902496338], [4.393196105957031], [5.3080644607543945], [6.0592041015625], [5.974006652832031], [6.332501411437988], [5.05901575088501], [5.626040458679199], [5.794736862182617], [5.251564025878906], [5.757691860198975], [4.960113048553467], [5.431746959686279], [5.003572940826416], [5.954739570617676], [5.458617210388184], [6.340101718902588], [5.516244411468506], [4.477784156799316], [4.935337066650391], [4.89360237121582], [5.76809024810791], [9.550930976867676], [5.363984107971191], [5.102375507354736], [5.764609336853027], [5.8076300621032715], [6.900912284851074], [5.512607097625732], [6.885948181152344], [6.206908702850342], [4.466306209564209], [4.7468037605285645], [6.5809807777404785], [5.472586631774902], [5.325417518615723], [5.997079849243164], [5.268748760223389], [4.621090888977051], [6.570944786071777], [5.349116802215576], [6.0963664054870605], [7.335054397583008], [4.806848526000977], [5.047961711883545], [5.914973735809326], [6.190097332000732], [5.275518417358398], [6.857922554016113], [4.819796562194824], [7.069564342498779], [6.213199138641357], [5.914973735809326], [5.020293235778809], [7.17845344543457], [5.320415496826172], [5.730303764343262], [5.980410575866699], [6.144803047180176], [6.231412410736084], [6.351831436157227], [6.041628360748291], [5.500179290771484], [4.973278999328613], [6.663186073303223], [5.284275054931641], [6.235599517822266], [6.228133201599121], [5.885361671447754], [6.625435829162598], [6.182233810424805], [5.545346260070801], [6.083103179931641], [6.699910640716553], [6.336609840393066], [4.935604095458984], [6.579122543334961], [6.962507247924805], [5.507875919342041], [5.8880157470703125], [6.353850364685059], [5.932641506195068], [6.187731742858887], [6.231132507324219], [5.206467628479004], [5.004029273986816], [6.714703559875488], [6.508650779724121], [5.187054634094238], [6.666534423828125], [6.3977952003479], [5.541524887084961], [6.652210235595703], [5.112921714782715], [6.109341621398926], [6.415482521057129], [6.11068058013916], [5.419198513031006], [5.0313825607299805], [5.740370750427246], [5.302877902984619], [4.986656188964844], [5.327646732330322], [6.564060211181641], [5.078128814697266], [5.51192569732666], [4.587516784667969], [6.561334133148193], [4.782570838928223], [5.249621391296387], [5.062320709228516], [5.709226608276367], [5.805761337280273], [5.292055130004883], [6.2233405113220215], [5.389714241027832], [5.003279209136963], [5.279829025268555], [6.1666741371154785], [6.366522789001465], [5.89241886138916], [5.497310638427734], [7.1521992683410645], [5.794652462005615], [5.474090576171875], [5.139674663543701], [5.069253921508789], [5.923877239227295], [6.195755958557129], [6.81646728515625], [5.805761337280273], [5.664484977722168], [5.163423538208008], [6.409900665283203], [5.23789119720459], [5.115969657897949], [4.912011623382568], [6.094397068023682], [6.831437110900879], [5.853494644165039], [5.260476589202881], [5.853592872619629], [5.308385372161865], [6.601244926452637], [6.47728157043457], [5.825009822845459], [6.226047039031982], [5.571065425872803], [5.44590425491333], [4.3864240646362305], [6.084000587463379], [5.360404014587402], [6.253697395324707], [6.290676116943359], [4.749328136444092], [5.110100746154785], [5.483736515045166], [7.6931352615356445], [6.360896587371826], [6.730966567993164], [5.185348987579346], [5.86604118347168], [5.050249099731445], [6.296051502227783], [7.16941499710083], [6.398283004760742], [5.506868362426758], [5.6904988288879395], [5.575429916381836], [6.432767868041992], [5.990605354309082], [5.706715106964111], [6.46299934387207], [5.426754474639893], [5.9361701011657715], [5.682673931121826], [6.4752092361450195], [6.006805419921875], [5.153072357177734], [5.675548553466797], [5.780808448791504], [6.280543327331543], [5.424032688140869], [5.258572578430176], [5.217790603637695], [6.598941802978516], [5.627253532409668], [4.972570896148682], [5.414769172668457], [5.975199222564697], [5.797441005706787], [5.575203895568848], [6.433839797973633], [6.87082576751709], [7.130575656890869], [6.1170220375061035], [4.806848526000977], [6.534673690795898], [6.021583080291748], [5.431746959686279], [6.2405686378479], [4.767961025238037], [5.241872787475586], [5.668504238128662], [6.316230773925781], [5.099889755249023], [6.120293140411377], [6.066538333892822], [6.595968246459961], [5.804475784301758], [4.969152450561523], [5.821369171142578], [6.045168876647949], [4.51273775100708], [5.606626510620117], [4.561766624450684], [4.747709274291992], [5.128079414367676], [5.7944536209106445], [5.327761173248291], [5.579089164733887], [5.980051040649414], [6.1405229568481445], [5.783078193664551], [4.9898681640625], [5.399312496185303], [5.706940174102783], [4.993546485900879], [6.039317607879639], [5.743836402893066], [6.491858959197998], [4.521795272827148], [5.022658348083496], [5.765023708343506], [5.6836323738098145], [5.961296081542969], [6.12979793548584], [6.076747894287109], [4.9880194664001465], [6.233462810516357], [5.329000949859619], [6.2001543045043945], [5.466865062713623], [5.716118812561035], [5.529762268066406], [5.405203342437744], [6.225823402404785], [4.863766670227051], [6.340101718902588], [6.372403621673584], [4.722380638122559], [5.661041736602783], [5.038768291473389], [5.733263969421387], [6.186014175415039], [5.261042594909668], [5.614664077758789], [5.63411283493042], [7.4250030517578125], [5.274466514587402], [5.602084636688232], [6.730008125305176], [5.549941539764404], [5.748468399047852], [5.747517108917236], [6.225688934326172], [4.61962890625], [5.253489017486572], [4.932491779327393], [5.217289447784424], [5.098508834838867], [4.499122142791748], [5.935341835021973], [5.127956390380859], [7.080028533935547], [5.84432315826416], [5.258655548095703], [4.738848686218262], [5.287318229675293], [5.357834815979004], [5.912421226501465], [4.693772315979004], [5.633632183074951], [5.109187126159668], [5.3624067306518555], [5.958525657653809], [5.361448287963867], [6.8436737060546875], [5.13685417175293], [6.0704755783081055], [7.326413631439209], [6.8400373458862305], [4.826947212219238], [7.130575656890869], [4.819699764251709], [6.045168876647949], [5.5005202293396], [6.203879356384277], [5.141256809234619], [6.759472846984863], [6.252534866333008], [5.362483024597168], [6.194631576538086], [6.661322593688965], [5.960886478424072], [6.490796089172363], [6.5879926681518555], [5.005964279174805], [5.760632514953613], [6.71362829208374], [5.473667144775391], [5.934514999389648], [5.799506187438965], [6.169588088989258], [5.30710506439209], [4.839699745178223], [5.22236967086792], [5.245821952819824], [6.3691277503967285], [5.987836837768555], [4.832630634307861], [5.823732852935791], [6.445571422576904], [7.241118431091309], [6.099580764770508], [7.318080902099609], [5.585807800292969], [5.179816246032715], [6.786224365234375], [6.201314926147461], [5.873574256896973], [5.663777828216553], [5.693248748779297], [5.998187065124512], [6.074760913848877], [6.630025386810303], [5.643471717834473], [6.0506439208984375], [5.527863025665283], [6.733603000640869], [4.648469924926758], [5.427380561828613], [5.48864221572876], [5.996903419494629], [4.979662895202637], [4.8602471351623535], [6.554563522338867], [5.165788650512695], [5.566655158996582], [6.006805419921875], [6.296051502227783], [4.846245288848877], [5.346356391906738], [5.953953742980957], [5.133893966674805], [6.894260406494141], [5.593039512634277], [6.1852288246154785], [5.912322044372559], [5.883569717407227], [8.231206893920898], [6.5271735191345215], [6.137930870056152], [5.3409037590026855], [6.2947306632995605], [5.053386211395264], [5.03670072555542], [4.429465293884277], [6.458412170410156], [5.201186180114746], [5.56595516204834], [6.63392972946167], [4.877532005310059], [7.984630584716797], [6.023756980895996], [5.217250347137451], [6.078798294067383], [5.19087028503418], [5.096920013427734], [6.252651214599609], [6.977356910705566], [5.461545944213867], [5.99232816696167], [5.309061050415039], [5.873188018798828], [5.8085713386535645], [5.211810111999512], [6.479748725891113], [5.860003471374512], [5.848025321960449], [5.934089660644531], [5.225558280944824], [6.034300804138184], [7.158854007720947], [6.047826766967773], [4.920107841491699], [5.329738140106201], [5.174809455871582], [5.04502010345459], [5.365536689758301], [5.394653797149658], [4.903367519378662], [6.103233337402344], [6.951519966125488], [5.638150215148926], [5.235992908477783], [6.4136881828308105], [5.773282051086426], [5.261275768280029], [6.340101718902588], [5.81879997253418], [5.490774631500244], [5.670197486877441], [5.399763107299805], [6.061166763305664], [5.624790668487549], [6.463038444519043], [6.190523147583008], [6.758662700653076], [6.904023170471191], [4.968358039855957], [5.658368110656738], [6.488691329956055], [5.3529253005981445], [5.629581451416016], [6.722248077392578], [6.5453009605407715], [7.460784912109375], [7.1912736892700195], [4.999486923217773], [5.851677894592285], [4.654998779296875], [5.477601051330566], [6.409723281860352], [5.689076900482178], [6.679937362670898], [5.112160682678223], [5.481213569641113], [5.458282470703125], [5.524418830871582], [4.382967948913574], [6.228708267211914], [6.259372711181641], [6.420805931091309], [5.925039291381836], [5.240242958068848], [6.515687942504883], [5.271002769470215], [5.298093318939209], [5.92887020111084], [6.169651031494141], [6.694385528564453], [5.615990161895752], [5.175333023071289], [5.902131080627441], [5.517093658447266], [5.99232816696167], [6.229768753051758], [5.411063194274902], [6.755311012268066], [5.835870742797852], [6.864997863769531], [4.823805809020996], [6.116300582885742], [5.536679267883301], [6.042814254760742], [5.801090240478516], [6.756678581237793], [5.66208553314209], [7.279626846313477], [5.527796268463135], [6.1260504722595215], [5.886814117431641], [5.782425880432129], [6.183356285095215], [4.824529647827148], [6.424945831298828], [6.248233795166016], [5.934796333312988], [4.986469268798828], [6.057347297668457], [5.9074015617370605], [4.976879596710205], [5.797441005706787], [5.41248893737793], [6.270365238189697], [7.205627918243408], [6.865533828735352], [5.478635787963867], [5.642178058624268], [4.8933210372924805], [6.137329578399658], [5.374831676483154], [5.8582353591918945], [5.510884761810303], [6.197856903076172], [5.654637336730957], [5.487861633300781], [5.2270097732543945], [5.402131080627441], [5.53907585144043], [8.070158004760742], [4.780088424682617], [6.468647480010986], [5.408500671386719], [5.920350074768066], [6.213913917541504], [5.099555015563965], [7.490298271179199], [5.9963531494140625], [6.384494781494141], [5.682950496673584], [5.817166328430176], [5.2099103927612305], [5.5395684242248535], [6.867364883422852], [6.3629302978515625], [6.45961856842041], [6.755311012268066], [4.978031158447266], [4.595622539520264], [6.462488651275635], [5.537236213684082], [5.695253372192383], [7.011445045471191], [5.895013809204102], [5.204665660858154], [5.646049499511719], [5.057344436645508], [5.709643840789795], [5.885168075561523], [6.023756504058838], [5.113542079925537], [6.941859245300293], [5.094321250915527], [6.4227213859558105], [6.061107635498047], [6.057514190673828], [5.51923131942749], [5.349977970123291], [5.34462833404541], [5.31485652923584], [5.031646251678467], [5.83493709564209], [7.21992301940918], [5.162097930908203], [4.850809097290039], [5.786752700805664], [5.721436023712158], [5.8814592361450195], [6.54141092300415], [5.5342559814453125], [6.952943325042725], [5.934782981872559], [6.404233932495117]]}"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "model_uri = 'runs:/53bd5d1d15d74d49b87cfebc084baa30/model'\n",
    "\n",
    "# Replace INPUT_EXAMPLE with your own input example to the model\n",
    "# A valid input example is a data instance suitable for pyfunc prediction\n",
    "input_data = X_test\n",
    "\n",
    "# Verify the model with the provided input data using the logged dependencies.\n",
    "# For more details, refer to:\n",
    "# https://mlflow.org/docs/latest/models.html#validate-models-before-deployment\n",
    "mlflow.models.predict(\n",
    "    model_uri=model_uri,\n",
    "    input_data=input_data,\n",
    "    env_manager=\"local\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b95ce72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
